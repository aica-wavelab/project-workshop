<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"><style type="text/css"> .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:not(.external):nth-child(5) > .nav-list-link { display: block; font-weight: 600; text-decoration: none; background-image: linear-gradient(-90deg, #ebedf5 0%, rgba(235, 237, 245, 0.8) 80%, rgba(235, 237, 245, 0) 100%); } .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:nth-child(5) > .nav-list-expander svg { transform: rotate(-90deg); } .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:nth-child(5) > .nav-list { display: block; } .site-nav > .nav-category-list > .nav-list-item > .nav-list-expander svg { transform: rotate(-90deg); } .site-nav > .nav-category-list > .nav-list-item > .nav-list { display: block; }</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-JFBL1M5RX8"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-JFBL1M5RX8', { 'anonymize_ip': true }); </script> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/assets/images/favicon.ico" type="image/x-icon"><title>Mapping by demonstration | AICA project workshop</title><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Mapping by demonstration" /><meta name="author" content="Téo Sanchez, Benedikt Zönnchen" /><meta property="og:locale" content="en_US" /><meta name="description" content="Artificial Intelligence in Culture and Arts (AICA): Tools and tutorials for the project workshop at the Wavelab" /><meta property="og:description" content="Artificial Intelligence in Culture and Arts (AICA): Tools and tutorials for the project workshop at the Wavelab" /><link rel="canonical" href="https://aica-wavelab.github.io/project-workshop/docs/2_mapping_demonstration" /><meta property="og:url" content="https://aica-wavelab.github.io/project-workshop/docs/2_mapping_demonstration" /><meta property="og:site_name" content="AICA project workshop" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Mapping by demonstration" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Téo Sanchez, Benedikt Zönnchen"},"description":"Artificial Intelligence in Culture and Arts (AICA): Tools and tutorials for the project workshop at the Wavelab","headline":"Mapping by demonstration","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://aica-wavelab.github.io/project-workshop/assets/images/logos/aica_logo.png"},"name":"Téo Sanchez, Benedikt Zönnchen"},"url":"https://aica-wavelab.github.io/project-workshop/docs/2_mapping_demonstration"}</script><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-logo" role="img" aria-label="AICA project workshop"></div>AICA project workshop </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">About</a><li class="nav-list-item"><a href="/docs/directions/" class="nav-list-link">Directions and useful infos</a><li class="nav-list-item"><a href="/docs/tutorial/" class="nav-list-link">Tutorial overview</a><li class="nav-list-item"><a href="/docs/1_intro_ml" class="nav-list-link">1. Introduction to machine learning</a><li class="nav-list-item"><a href="/docs/2_mapping_demonstration" class="nav-list-link">2. Mapping by demonstration</a><li class="nav-list-item"><a href="/docs/3_audio_modelling" class="nav-list-link">3. Audio modelling and synthesis with neural networks</a><li class="nav-list-item"><a href="/docs/4_embed_cultural_archives" class="nav-list-link">4. Embed and explore cultural archives</a><li class="nav-list-item"><a href="/docs/5_language_model" class="nav-list-link">5. Language model for domain-specific application</a><li class="nav-list-item"><a href="/docs/staff/" class="nav-list-link">Instructors</a><li class="nav-list-item"><a href="/docs/credits/" class="nav-list-link">Tools and credits</a></ul></nav><footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search" role="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search AICA project workshop" aria-label="Search AICA project workshop" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="https://www.wavelab.io/aica/" class="site-button" > AICA Digitization College </a></ul></nav></div><div class="main-content-wrap"><div id="main-content" class="main-content"><main><h1 class="no_toc" id="mapping-by-demonstration"> <a href="#mapping-by-demonstration" class="anchor-heading" aria-labelledby="mapping-by-demonstration"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Mapping by demonstration</h1><p>A computer program is a sequence instructions for a computer to execute. Programs often take inputs, process them, and produce outputs. The way inputs are processed are usually explicited by the human programmer, using a programming language. Machine learning (ML) offers an alternative to explicit programmation: ML algorithms can learn to process data from examples. The human developper provides pairs of input and its corresponding output, and the ML model is then optimized to reproduce the mapping from the example provided. We call this approach <strong>mapping by demonstration</strong>, and finds many applications in the creative and cultural industries: in performing arts (gesture to sound), in video games, in robotics, among others.</p><p>This chapter will teach you how to build a real-time mapping from gesture to sound using ML and <a href="https://puredata.info/">Pure Data</a> (Pd), a free an open-source <strong>visual programming language</strong> for creating interactive computer music and multimedia works. In particular, this chapter will teach you how to use your smartphone as a gesture sensor device, build a minimal sound synthesizer in Pd, and train a ML model to <strong>control the synthesizer from gestures you choose!</strong></p><p>Second, you will learn how to do the same using the Wekinator, a free, open source software that allows anyone to use machine learning to map arbitrary OSC messages.</p><hr /><h2 class="no_toc text-delta" id="table-of-contents"> <a href="#table-of-contents" class="anchor-heading" aria-labelledby="table-of-contents"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Table of contents</h2><ol id="markdown-toc"><li><a href="#use-your-phone-as-a-sensor-device" id="markdown-toc-use-your-phone-as-a-sensor-device">Use your phone as a sensor device</a><ol><li><a href="#send-osc-messages-from-your-phone" id="markdown-toc-send-osc-messages-from-your-phone">Send OSC messages from your phone</a><li><a href="#receive-osc-messages-on-your-computer-in-pure-data" id="markdown-toc-receive-osc-messages-on-your-computer-in-pure-data">Receive OSC messages on your computer in Pure Data</a></ol><li><a href="#build-a-minimal-sound-synthesizer" id="markdown-toc-build-a-minimal-sound-synthesizer">Build a minimal sound synthesizer</a><li><a href="#map-sensors-to-synthesis-parameters-with-ml-lib-in-pure-data" id="markdown-toc-map-sensors-to-synthesis-parameters-with-ml-lib-in-pure-data">Map sensors to synthesis parameters with ml-lib in Pure Data</a><li><a href="#map-sensors-to-synthesis-parameters-with-wekinator" id="markdown-toc-map-sensors-to-synthesis-parameters-with-wekinator">Map sensors to synthesis parameters with Wekinator</a></ol><hr /><h2 id="use-your-phone-as-a-sensor-device"> <a href="#use-your-phone-as-a-sensor-device" class="anchor-heading" aria-labelledby="use-your-phone-as-a-sensor-device"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Use your phone as a sensor device</h2><p>Smartphones are excellent sensor devices because they include a large array of built-in sensors. For motion, they are equipped with accelerometers, gyroscopes, and step detection sensors. For geolocalisation, they are equipped with geopositioning, magnetic field detection, and light sensors. For touch detection, most touch screen enable precise multi-finger detection.</p><p>Additionally, smartphones are widely accessible and come with the advantage of wireless connectivity, allowing for seamless data transmission without the need for physical cables. Their portability ensures they can be used in various settings, especially in performance-based contexts where mobility is essential. Moreover, smartphones have significant computing power, enabling them to not only collect but also process complex data in real-time.</p><p>Open Sound Control (OSC) serves as an effective way to stream this sensor data. OSC is a communication protocol initially designed for networking sound synthesizers, computers, and other multimedia devices, offering more flexibility and a higher level of organization than traditional MIDI protocols. OSC signals can carry a variety of data types and are sent over standard network protocols like UDP or TCP. This compatibility with modern networking technologies makes OSC an ideal choice for transmitting the rich sensor data collected by smartphones. In creative and technical applications, OSC allows for this data to be streamed in real-time, enabling dynamic and interactive experiences.</p><p>An Open Sound Control (OSC) message is structured to include an address pattern followed by typed arguments. Here’s a simple example of an OSC message:</p><ul><li>Address Pattern: /filter/frequency<li>Arguments: 440.0</ul><p>In this example, /filter/frequency is the address pattern that indicates where the message is destined within the OSC namespace. It’s akin to a URL path in web development, pointing to a specific function or parameter in the receiving device or software.</p><p>The argument 440.0 is the value that is being sent to the specified address. In this context, it could represent a frequency value in Hertz that is being sent to a sound synthesizer’s filter frequency parameter.</p><p>The OSC message is compact and efficient, capable of supporting multiple arguments of different types (like integers, floats, strings, etc.) following the address pattern.</p><h3 id="send-osc-messages-from-your-phone"> <a href="#send-osc-messages-from-your-phone" class="anchor-heading" aria-labelledby="send-osc-messages-from-your-phone"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Send OSC messages from your phone</h3><p>Free mobile applications exist to stream sensor data from your phone. We recommend:</p><ul><li><a href="https://sensors2.org/osc/">Sensors2OCS</a> (Android) allowing to stream all the sensors of your phone as OSC messages<li><a href="https://play.google.com/store/apps/details?id=com.ffsmultimedia.osccontroller&amp;hl=de">Osc controller</a> (Android) that proposes generic button and slider interfaces to send OSC messages</ul><p>To send OSC messages from your phone to your computer, you need to connect your phone and your computer to the same network. You can either use a local network (wifi) or a global network (internet). In both cases, you need to know the IP address of your computer. You can find it by typing <code class="language-plaintext highlighter-rouge">ipconfig</code> in the terminal (MacOS) or <code class="language-plaintext highlighter-rouge">ipconfig</code> in the command prompt (Windows), or in the network settings of your computer.</p><p>Both application should be configurated with the IP address of your computer and the port number to send OSC messages to. The port number is a number between 0 and 65535. We recommend to use a number between 8000 and 9000.</p><h3 id="receive-osc-messages-on-your-computer-in-pure-data"> <a href="#receive-osc-messages-on-your-computer-in-pure-data" class="anchor-heading" aria-labelledby="receive-osc-messages-on-your-computer-in-pure-data"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Receive OSC messages on your computer in Pure Data</h3><p><a href="https://puredata.info/">Pure Data</a> is a free and open-source visual programming language for creating interactive computer music and multimedia works. Pd enables musicians, visual artists, performers, researchers, and developers to create software graphically, without writing lines of code. Pd is used to process and generate sound, video, 2D/3D graphics, and interface sensors, input devices, and MIDI. Pd can easily run on micro-computers such as Raspberry Py and is hence a great tool for prototyping interactive systems. We recommend using the <a href="https://plugdata.org/">Plug Data</a> version of Pd, as it provides a more user-friendly interface, and can be used as a standalone app or as a VST3, LV2, CLAP or AU plugin.</p><p>You can download below a Pd patch we wrote to receive OSC messages from your phone. You can open it with Plug Data.</p><p><a href="https://github.com/aica-wavelab/aica-project-workshop/tree/main/2_mapping_demonstration" class="btn" target="_blank">2a_phone_sensors.pd</a></p><p><img src="/assets/images/tutorials/2a_phone_sensors.png" alt="2a_phone_sensors" /></p><p>Let’s go through the patch. The <code class="language-plaintext highlighter-rouge">loadband</code> object send a <code class="language-plaintext highlighter-rouge">bang</code> message (trigger signal) when the Pd patch is loaded. It is generally used to initialize settings. The <code class="language-plaintext highlighter-rouge">listen 8000</code> object is a message triggered by the <code class="language-plaintext highlighter-rouge">loadbang</code>. It indicates that the program will listen for incoming OSC messages on port 8000. <code class="language-plaintext highlighter-rouge">netreceive -u -b</code> is configured to receive network messages using UDP protocol (-u for UDP and -b for binding to a port, which is set at 8000 from the previous object). Hence, the <code class="language-plaintext highlighter-rouge">netreceive</code> object will receive all raw messages, including those formatted in OSC, sent to port 8000. <code class="language-plaintext highlighter-rouge">oscparse</code> takes the raw OSC messages received from netreceive and parses them into a format that can be understood and used in Pd. Note that <code class="language-plaintext highlighter-rouge">oscparse</code> belongs to an external library named <code class="language-plaintext highlighter-rouge">osc</code> that you can install with the external manager named <code class="language-plaintext highlighter-rouge">deken</code>. The deken is a repository of all available external developed for Pd, and allow you to quickly install their last version from within Pd. You can access it from the menu <code class="language-plaintext highlighter-rouge">Settings/Externals</code>. Search for <code class="language-plaintext highlighter-rouge">osc</code> and install the library.</p><p><img src="/assets/images/tutorials/deken.png" alt="deken" /></p><p>At this stage, you should be able to receive OSC messages from your phone in Pd. You can also use the <code class="language-plaintext highlighter-rouge">print</code> object to display incomming messages in the console and check that you received the formatted OSC messages correctly.</p><p>The rest is just a matter of extracting the information you need from the OSC messages: route the numerical values (arguments) from their address pattern, process, and scale the numerical values to fit your needs. The two applications might have a different OSC message format (might add a keyword in the address pattern, or send the values in a different order), so you might need to adapt the patch to your needs.</p><h2 id="build-a-minimal-sound-synthesizer"> <a href="#build-a-minimal-sound-synthesizer" class="anchor-heading" aria-labelledby="build-a-minimal-sound-synthesizer"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Build a minimal sound synthesizer</h2><p>Now we can stream sensor data from our phone to Pd, we can for exemple track and record motion gestures. Let’s now build a sound synthesizer that we will control from the gestures.</p><p><a href="https://github.com/aica-wavelab/aica-project-workshop/tree/main/2_mapping_demonstration" class="btn" target="_blank">2b_sound_synthesis.pd</a></p><p><img src="/assets/images/tutorials/2b_sound_synthesis.png" alt="2b_sound_synthesis" /></p><p>Many techniques exist to synthesize sound. In this example, we will use substractive synthesis. The principle is to start from a complex sound (e.g., white noise) and remove some of its harmonics using a filter, in order to obtain a simpler sound.</p><p>The <code class="language-plaintext highlighter-rouge">noise~</code> object generates white noise. The <code class="language-plaintext highlighter-rouge">~</code> indicates that the object is a signal object (rather than a message), i.e., it processes audio signals. The <code class="language-plaintext highlighter-rouge">vcf~</code> object is a voltage-controlled filter. It takes the white noise as input and two filter paramters:</p><ol><li>The cut-off frequency (in Hz) , which is the frequency above which the harmonics are removed.<li>The Q factor, which is the resonance of the filter. The higher the Q factor, the more the harmonics are removed.</ol><p>Both parameters are controled using horizontal sliders.</p><blockquote><p class="warning">Be sure to turn the volume of your computer down before playing with the sliders, as the sound can be very loud!</p></blockquote><p>The output of the filter is sent to the <code class="language-plaintext highlighter-rouge">dac~</code> object, which is the digital-to-analog converter that converts the digital signal into an analog signal that can be played by your speakers. You can activate the DSP (digital signal processing) by clicking on the <code class="language-plaintext highlighter-rouge">⏼</code> button on the bottom right corner of your screen.</p><h2 id="map-sensors-to-synthesis-parameters-with-ml-lib-in-pure-data"> <a href="#map-sensors-to-synthesis-parameters-with-ml-lib-in-pure-data" class="anchor-heading" aria-labelledby="map-sensors-to-synthesis-parameters-with-ml-lib-in-pure-data"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Map sensors to synthesis parameters with ml-lib in Pure Data</h2><p>At this stage, we successfully received OSC messages from a smartphone and created a simple sound synthesizer controlled by two parameters (cut-off frequency and Q value). However, we still do not have a mapping between the OSC messages and the synthesis parameters and programming such a mapping by hand can be tedious and time-consuming. We will now use machine learning to learn this mapping from examples.</p><p>You can download below the Pd patch to train a ML model to map OSC messages to the cut-off frequency and Q value of the filter.</p><p><a href="https://github.com/aica-wavelab/aica-project-workshop/tree/main/2_mapping_demonstration" class="btn" target="_blank">2c_ml_regression_mapping.pd</a></p><p><img src="/assets/images/tutorials/2c_ml_regression_mapping.png" alt="2c_ml_regression_mapping" /></p><p>This patch require to install the ml-lib library. Unfortunately, this library is not up-to-date in the Deken, so you will have to install it manually. You can find the latest releases of the library at this address:</p><p><a href="https://github.com/irllabs/ml-lib/releases" class="btn" target="_blank">ml-lib</a></p><p>Once downloaded, copy paste the folter <code class="language-plaintext highlighter-rouge">ml.lib</code> in the folder <code class="language-plaintext highlighter-rouge">Library/plugdata/Library/Extra</code> (MacOS) or <code class="language-plaintext highlighter-rouge">PlugData/extra</code> (Windows) and reboot Plug Data.</p><p>You should now be able to load various machine learning classifier or regressor. We will use the <code class="language-plaintext highlighter-rouge">ml.ann</code> which is a generalist artificial neural network.</p><p>First, we must conduct data collection, e.g, gathering input (gesture data) and their corresponding outputs (synthesizer parameters). Open both patches <code class="language-plaintext highlighter-rouge">2a_phone_sensors.pd</code>, <code class="language-plaintext highlighter-rouge">2b_sound_synthesis.pd</code>, and check that your patch <code class="language-plaintext highlighter-rouge">2c_ml_regression_mapping.pd</code> receive the signals from both the phone and the synthesizer.</p><p>Then, click on the radio button and select the middle one (data collection). This will start the recording of the data in the <code class="language-plaintext highlighter-rouge">ml.ann</code> object. You can record as many examples as you want, while changing the position of the phone, and the values of the sound synthesizer.</p><p>When you are done, click on the <code class="language-plaintext highlighter-rouge">train</code> message to start training your artificial neural network. Check the console to see if there’s any error messages.</p><p>Once the model is trained, click on the third mode (right button) to start the prediction. You should now be able to control the sound synthesizer from the OSC messages sent by your phone.</p><h2 id="map-sensors-to-synthesis-parameters-with-wekinator"> <a href="#map-sensors-to-synthesis-parameters-with-wekinator" class="anchor-heading" aria-labelledby="map-sensors-to-synthesis-parameters-with-wekinator"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Map sensors to synthesis parameters with Wekinator</h2><p>The Wekinator is a free and standalone software that allows to do the exact same steps as in the patch <code class="language-plaintext highlighter-rouge">2c_ml_regression_mapping.pd</code> but in a more user-friendly interface. The wekinator accept and send arbitrary OSC messages.</p><p>You can download the Wekinator at this address:</p><p><a href="http://www.wekinator.org/" class="btn" target="_blank">Wekinator</a></p><p>Try to train a new mapping using the Wekinator and the <code class="language-plaintext highlighter-rouge">2a_phone_sensors.pd</code> and <code class="language-plaintext highlighter-rouge">2b_sound_synthesis.pd</code> patches.</p><p>Dr. Benedikt Zönnchen provided a live demonstration of the wekinator using <a href="https://processing.org/">Processing</a> (input) and <a href="https://supercollider.github.io/">SuperCollider</a> (output). You can find the code and the video of the demonstration at this address:</p><p><a href="https://syncandshare.lrz.de/getlink/fiByPXEJ9rS4yR42qToaSr/presentations_aica_2023.zip" class="btn" target="_blank">Slides: replacing code with machine learning</a> <a href="https://syncandshare.lrz.de/getlink/fiSxeku21dD6oZ2WZagVKW/" class="btn" target="_blank">Code: replacing code with machine learning</a></p></main><hr><footer><p><a href="#top" id="back-to-top">Back to top</a></p><div class="logo-container"><footer> <img src="/assets/images/logos/hm_logo.png" alt="Logo HM"> <img src="/assets/images/logos/mucdai_logo.jpg" alt="Logo MUC.DAI"> <img src="/assets/images/logos/hmtm_logo.png" alt="Logo HMTM"> <img src="/assets/images/logos/wavelab_logo.jpg" alt="Logo Wavelab"> <img src="/assets/images/logos/bidt_logo.png" alt="Logo BIDT"></footer></div><style> .logo-container img { filter: grayscale(100%); width: 100px; margin-right: 50px; } .logo-container img:last-child { margin-right: 0; }</style><div class="d-flex mt-2"></div></footer></div></div><div class="search-overlay"></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@9.1.6/dist/mermaid.min.js"></script> <script> var config = {} ; mermaid.initialize(config); window.mermaid.init(undefined, document.querySelectorAll('.language-mermaid')); </script>

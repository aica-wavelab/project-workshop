<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JFBL1M5RX8"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-JFBL1M5RX8', { 'anonymize_ip': true }); </script> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/assets/images/favicon.ico" type="image/x-icon"><title>Group feedback | AICA project workshop</title><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Group feedback" /><meta name="author" content="Téo Sanchez, Benedikt Zönnchen" /><meta property="og:locale" content="en_US" /><meta name="description" content="Artificial Intelligence in Culture and Arts (AICA): Tools and tutorials for the project workshop at the Wavelab" /><meta property="og:description" content="Artificial Intelligence in Culture and Arts (AICA): Tools and tutorials for the project workshop at the Wavelab" /><link rel="canonical" href="https://aica-wavelab.github.io/project-workshop/docs/feedback/group_feedback/" /><meta property="og:url" content="https://aica-wavelab.github.io/project-workshop/docs/feedback/group_feedback/" /><meta property="og:site_name" content="AICA project workshop" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Group feedback" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Téo Sanchez, Benedikt Zönnchen"},"description":"Artificial Intelligence in Culture and Arts (AICA): Tools and tutorials for the project workshop at the Wavelab","headline":"Group feedback","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://aica-wavelab.github.io/project-workshop/assets/images/logos/aica_logo.png"},"name":"Téo Sanchez, Benedikt Zönnchen"},"url":"https://aica-wavelab.github.io/project-workshop/docs/feedback/group_feedback/"}</script><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-logo" role="img" aria-label="AICA project workshop"></div>AICA project workshop </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">About</a><li class="nav-list-item"><a href="/docs/directions/" class="nav-list-link">Directions and useful infos</a><li class="nav-list-item"><a href="/docs/tutorial/" class="nav-list-link">Tutorial overview</a><li class="nav-list-item"><a href="/docs/1_intro_ml" class="nav-list-link">1. Introduction to machine learning</a><li class="nav-list-item"><a href="/docs/2_mapping_demonstration" class="nav-list-link">2. Mapping by demonstration</a><li class="nav-list-item"><a href="/docs/3_audio_modelling" class="nav-list-link">3. Audio modelling and synthesis with neural networks</a><li class="nav-list-item"><a href="/docs/4_embed_cultural_archives" class="nav-list-link">4. Embed and explore cultural archives</a><li class="nav-list-item"><a href="/docs/5_language_model" class="nav-list-link">5. Language model for domain-specific application</a><li class="nav-list-item"><a href="/docs/staff/" class="nav-list-link">Instructors</a><li class="nav-list-item"><a href="/docs/credits/" class="nav-list-link">Tools and credits</a></ul></nav><footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search" role="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search AICA project workshop" aria-label="Search AICA project workshop" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="https://www.wavelab.io/aica/" class="site-button" > AICA Digitization College </a></ul></nav></div><div class="main-content-wrap"><div id="main-content" class="main-content"><main><p><a href="/assets/pdf/group_feedback_notes.pdf" class="btn btn-purple fs-5 mb-4 mb-md-0 mr-2">Raw notes (sent by email)</a></p><h2 id="group-a--human-machine-co-improvisation"> <a href="#group-a--human-machine-co-improvisation" class="anchor-heading" aria-labelledby="group-a--human-machine-co-improvisation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Group A : Human-machine co-improvisation</h2><ul><li>An implementation of a <a href="https://github.com/ajwnycct/factorOracle-Pd">factor oracle for Pure Data</a><li><a href="http://recherche.ircam.fr/equipes/repmus/OMax/">Omax</a><li><a href="https://forum.ircam.fr/projects/detail/dicy2/">Dicy2</a> and its <a href="https://www.youtube.com/watch?v=xt8-rlqMIQM&amp;ab_channel=Ircam">video tutorials</a><li><a href="https://improtech.ircam.fr/">Festival on co-improvisation</a><li>Valerio Velardo <a href="https://www.youtube.com/@ValerioVelardoTheSoundofAI">Youtube channel</a> on machine learning for music<li><a href="https://www.artemigioti.com/about.html">Artemi-Maria Gioti</a> was also interested in machine-artist communication and gave a talk in the last Wintersemester for AICA and <a href="https://www.youtube.com/watch?v=r5XixsSdGKI&amp;ab_channel=FluidCorpusManipulation">here</a> is also an interview where she talks about some implementation details.</ul><blockquote><p>Assayag, G., &amp; Dubnov, S. (2004). Using factor oracles for machine improvisation. Soft Computing, 8(9), 604-610. Wilson, A. J. (2016). factorOracle: an Extensible Max External for Investigating Applications of the Factor Oracle Automaton in Real-Time Music Improvisation.</p></blockquote><blockquote><p>Nika, J., Chemillier, M., &amp; Assayag, G. (2017). Improtek: introducing scenarios into human-computer music improvisation. Computers in Entertainment (CIE), 14(2), 1-27.</p></blockquote><blockquote><p>Dubnov, S., Assayag, G., &amp; Cont, A. (2007). Audio oracle: A new algorithm for fast learning of audio structures. In Proceedings of International Computer Music Conference (ICMC). ICMA.</p></blockquote><h2 id="group-b--collaborative-visual-story-telling"> <a href="#group-b--collaborative-visual-story-telling" class="anchor-heading" aria-labelledby="group-b--collaborative-visual-story-telling"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Group B : Collaborative visual story-telling</h2><ul><li><a href="https://www.assemblyai.com/blog/how-to-run-stable-diffusion-locally-to-generate-images/">How to use Stable diffusion locally</a><li><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">Automatic111 webGUI</a> : AUTOMATIC111 also provides an API so it should be possible to<ul><li>Make a history of prompts via some GUI, for example a Python application<li>Send parts of the history as prompt to Stable Diffusion<li>Receive and display the image and save the pair (image, prompt) into the archive/history</ul></ul><p>You can also access Stable Diffusion and other image generators via an API but if it does not run on your machine you have to pay for it.</p><h2 id="group-c--emotion-recognition-for-personalized-recommendation"> <a href="#group-c--emotion-recognition-for-personalized-recommendation" class="anchor-heading" aria-labelledby="group-c--emotion-recognition-for-personalized-recommendation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Group C : Emotion recognition for personalized recommendation</h2><ul><li><a href="https://github.com/justinshenk/fer">FER</a><li><a href="https://github.com/serengil/deepface">DeepFace</a><li><a href="https://facework.app/">FaceWork</a> : a critical gamification of facial recognition. You can read more about the artist Kyle McDonald that is familiar with the relation between <a href="https://kcimc.medium.com/working-with-faces-e63a86391a93">AI and faces</a>.</ul><h2 id="group-d--virtual-archives-of-clothing-artefacts"> <a href="#group-d--virtual-archives-of-clothing-artefacts" class="anchor-heading" aria-labelledby="group-d--virtual-archives-of-clothing-artefacts"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Group D : Virtual archives of clothing artefacts</h2><ul><li><a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">Gaussian splatting</a> for 3D modelling of items<li><a href="https://www.kaggle.com/code/eliotbarr/fashion-mnist-tutorial">A tutorial</a> for learning embeddings of (simplistic) fashion item images.<li><a href="https://github.com/google-coral/project-posenet">PoseNet</a> for estimating poses</ul><blockquote><p>Gu, X., Wong, Y., Shou, L., Peng, P., Chen, G., &amp; Kankanhalli, M. S. (2018). Multi-modal and multi-domain embedding learning for fashion retrieval and analysis. IEEE Transactions on Multimedia, 21(6), 1524-1537.</p></blockquote><h2 id="group-e--ai-augmented-audio-guide"> <a href="#group-e--ai-augmented-audio-guide" class="anchor-heading" aria-labelledby="group-e--ai-augmented-audio-guide"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Group E : AI-augmented audio guide</h2><ul><li>Audio-to-text : <a href="https://github.com/mozilla/DeepSpeech">DeepSpeech</a> (can run locally) or <a href="https://openai.com/research/whisper">Whisper</a> (API calls);<li>Question processing : <a href="https://www.langchain.com/">Langchain</a> or <a href="https://www.llamaindex.ai/">https://www.llamaindex.ai/</a>. My guess is that you will necessary have to call an API for this part;<li>Have a look to <a href="https://www.youtube.com/@chatwithdata">Chat with data</a> and <a href="https://github.com/ksm26/LangChain-Chat-with-Your-Data">other tutorials</a>. Local Large Language Models exist (<a href="https://python.langchain.com/docs/integrations/llms/llamacpp">llama.cpp</a>) but are heavy to be supported by an embed system like a Raspberry Pi (it is already too heavy for my computer);<li>Text-to-speech : I don’t know much about it but have a look to <a href="https://github.com/mozilla/TTS">Mozilla TTS</a>, <a href="https://gtts.readthedocs.io/en/latest/">Google TTS</a>, and <a href="https://github.com/coqui-ai/TTS">coqui-ai TTS</a>.</ul></main><hr><footer><p><a href="#top" id="back-to-top">Back to top</a></p><div class="logo-container"><footer> <img src="/assets/images/logos/hm_logo.png" alt="Logo HM"> <img src="/assets/images/logos/mucdai_logo.jpg" alt="Logo MUC.DAI"> <img src="/assets/images/logos/hmtm_logo.png" alt="Logo HMTM"> <img src="/assets/images/logos/wavelab_logo.jpg" alt="Logo Wavelab"> <img src="/assets/images/logos/bidt_logo.png" alt="Logo BIDT"></footer></div><style> .logo-container img { filter: grayscale(100%); width: 100px; margin-right: 50px; } .logo-container img:last-child { margin-right: 0; }</style><div class="d-flex mt-2"></div></footer></div></div><div class="search-overlay"></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@9.1.6/dist/mermaid.min.js"></script> <script> var config = {} ; mermaid.initialize(config); window.mermaid.init(undefined, document.querySelectorAll('.language-mermaid')); </script>
